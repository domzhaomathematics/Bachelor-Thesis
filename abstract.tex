\begin{abstract}
  The ability to fast generalize previous knowledge and learn novel tasks from it just from a small set of examples has been a , in the past few years, subject undergoing intense study from the Machine Learning field and is considered the one of the next milestones in the pursuit of general artificial intelligence. In this mathematics bachelor thesis, after a comprehensive literature review, a probabilistic interpretation and motivations, we will propose a novel soft parameter sharing model in the context of meta-learning to achieve better results than its normal layers counterparts. The soft parameters sharing methods enables blablabla. We conduct extensive experiments on two standard datasets for few-shots learning, namely miniImageNet and Omniglot.
  
\end{abstract}
